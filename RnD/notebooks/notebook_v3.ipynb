{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "libpath = \"../../scripts\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(libpath)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tsfresh import (extract_features, select_features)\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import settings\n",
    "from tsfresh.transformers import RelevantFeatureAugmenter, FeatureAugmenter\n",
    "from tsfresh.feature_extraction.settings import MinimalFCParameters, EfficientFCParameters, ComprehensiveFCParameters\n",
    "from tsfresh.feature_extraction.settings import from_columns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from preprocessing import TsfreshDatasetTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dasasets(set_of_datasets):\n",
    "    # Соединяет несколько датасетов\n",
    "    concated_dataframe = pd.DataFrame([])\n",
    "    concated_labels = []\n",
    "    current_id = 0\n",
    "    for (name, data, label) in set_of_datasets:\n",
    "        data_copy = data.copy()\n",
    "        data_copy[\"id\"] += current_id\n",
    "        concated_dataframe = pd.concat([concated_dataframe, data_copy], ignore_index=True)\n",
    "        current_id = concated_dataframe[\"id\"].iloc[-1] +1\n",
    "        concated_labels.extend(label)\n",
    "    return concated_dataframe, pd.Series(concated_labels)\n",
    "\n",
    "def rename_idxs(array):\n",
    "    # Переименовывает колонку id после удаления метки\n",
    "    a = array.copy()\n",
    "    a = np.array(a)\n",
    "    i = 1\n",
    "    while i < len(a):\n",
    "        if a[i] - a[i-1] > 1:\n",
    "            j = i\n",
    "            constant = a[j]\n",
    "            while j < len(a) and a[j] == constant:\n",
    "                a[j] = a[i-1] + 1\n",
    "                j += 1\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    if a[0] > 0:\n",
    "        a -= a[0]\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found unknown label in zone 455 \n",
      "date: 22/11/2024 14:02:36\n",
      "Found unknown label in zone 455 \n",
      "date: 09/12/2024 14:29:34\n",
      "Found unknown label in zone 455 \n",
      "date: 09/12/2024 09:49:55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Конкатенируем данные с нескольких объектов в один объект\n",
    "\n",
    "dataset_path_1 = \"/home/drozdovmk/Projects/ZB/zb-classification/data/data_markup/Classifier_cesis_12_11\"\n",
    "dataset_path_2 = \"/home/drozdovmk/Projects/ZB/zb-classification/data/data_markup/Classifier_demostend_18_11\"\n",
    "dataset_path_3 = \"/home/drozdovmk/Projects/ZB/zb-classification/data/data_markup/hdf5_adaptive\"\n",
    "dataset_path_4 = \"/home/drozdovmk/Projects/ZB/zb-classification/data/data_markup/Classifier_demostend_10_12_24\"\n",
    "data_getter = TsfreshDatasetTransformer()\n",
    "\n",
    "data_hdf5_long_1, label_hdf5_1 = data_getter.make_tsfresh_structure_from_nested_directory(dataset_path_1)\n",
    "data_hdf5_long_2, label_hdf5_2 = data_getter.make_tsfresh_structure_from_nested_directory(dataset_path_2)\n",
    "data_hdf5_long_3, label_hdf5_3 = data_getter.make_tsfresh_structure_from_simple_directory(dataset_path_3)\n",
    "data_hdf5_long_4, label_hdf5_4 = data_getter.make_tsfresh_structure_from_nested_directory(dataset_path_4)\n",
    "\n",
    "set_of_datasets = (\n",
    "    (\"cesis_data_nested\", data_hdf5_long_1, label_hdf5_1),\n",
    "    (\"demostend_data_nested\", data_hdf5_long_2, label_hdf5_2),\n",
    "    (\"demostend_data_simple\", data_hdf5_long_3, label_hdf5_3),\n",
    "    (\"demostend_device\", data_hdf5_long_4, label_hdf5_4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wind          698\n",
       "device        542\n",
       "hit           370\n",
       "unknown       236\n",
       "perelaz       186\n",
       "saw           151\n",
       "hit_z         109\n",
       "hit_g         109\n",
       "hit_series      9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# объединяем все датасеты в один\n",
    "data_hdf5, label_hdf5 = concat_dasasets(set_of_datasets)\n",
    "label_hdf5.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заменяем имена меток\n",
    "label_hdf5.replace([\"hit_g\", \"hit_z\"], \"hit\", inplace=True)\n",
    "\n",
    "# удалить метки из спика\n",
    "TO_DROP = [\"unknown\", \"hit_series\",]\n",
    "mask_drop = label_hdf5.isin(TO_DROP)\n",
    "drop_idxs =  label_hdf5[mask_drop].index # id of drop labels\n",
    "\n",
    "data_hdf5.drop(\n",
    "    index = data_hdf5[data_hdf5[\"id\"].isin(drop_idxs)].index,\n",
    "    inplace=True\n",
    ")\n",
    "label_hdf5.drop(\n",
    "    index = drop_idxs,\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "# min_count = label_hdf5.value_counts().min()\n",
    "# balanced_indexes = (label_hdf5.groupby(label_hdf5).\n",
    "#                    apply(lambda x: x.sample(min_count))).reset_index()[\"level_1\"]\n",
    "# data_hdf5 = data_hdf5[data_hdf5[\"id\"].isin(balanced_indexes)]\n",
    "# label_hdf5 = label_hdf5.loc[balanced_indexes]\n",
    "\n",
    "# Переименовываем колонку id (она изменилась после удаления меток из списка)\n",
    "\n",
    "\n",
    "data_hdf5[\"id\"] = rename_idxs(data_hdf5[\"id\"])\n",
    "\n",
    "data_hdf5.reset_index(drop=True, inplace=True)\n",
    "label_hdf5.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FilterColumnCreator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, transformations=None):\n",
    "        self.transformations = transformations\n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "    def transform(self, X:pd.DataFrame, transformations=None):\n",
    "        if self.transformations==None:\n",
    "            return X\n",
    "        X_transformed = X.copy()\n",
    "        for name, transformation in self.transformations:\n",
    "            X_transformed[name] = (\n",
    "                X_transformed.\n",
    "                groupby(\"id\")[\"signal_raw\"].\n",
    "                transform(transformation)\n",
    "            )\n",
    "        return X_transformed\n",
    "\n",
    "class FourierColumnCreator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, augmenter=None):\n",
    "        self.augmenter = augmenter\n",
    "        pass\n",
    "    def binned_fourier_transform(self, signal, n_bins=70, fs=1000):\n",
    "        N = len(signal)\n",
    "        Ts = 1/fs\n",
    "        fftfreqz = np.fft.fftfreq(N, Ts)[:N//2]\n",
    "        signal_fft = 2/N * np.abs(np.fft.fft(signal))[:N//2]\n",
    "        bin_edges = np.logspace(start=np.log2(fftfreqz[1]),\n",
    "                                stop=np.log2(fftfreqz[-1]),\n",
    "                                num=n_bins+1,\n",
    "                                base=2)\n",
    "        binned_fft = []\n",
    "        binned_freqz = []\n",
    "        for i in range(n_bins):\n",
    "            bin_mask = (fftfreqz >= bin_edges[i]) & (fftfreqz < bin_edges[i + 1])\n",
    "            if np.any(bin_mask):\n",
    "                binned_fft.append(np.mean(signal_fft[bin_mask]))\n",
    "                binned_freqz.append(np.mean(fftfreqz[bin_mask]))\n",
    "        return np.array(binned_fft)\n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X:pd.DataFrame):\n",
    "        result_df = pd.DataFrame()\n",
    "        indexes = X[\"id\"].unique()\n",
    "        result = (\n",
    "            X.groupby(\"id\")\n",
    "            [\"signal_raw\"].\n",
    "            apply(func=lambda x : self.binned_fourier_transform(\n",
    "                signal=x,\n",
    "                n_bins=100,\n",
    "                fs=1000,\n",
    "                ))\n",
    "            )\n",
    "        for idx in indexes:\n",
    "            temp_data = result[idx]\n",
    "            temp_df = pd.DataFrame({\n",
    "                'id': [int(idx)] * len(temp_data),\n",
    "                'freq_num': range(len(temp_data)),\n",
    "                'signal_binned_fft': temp_data\n",
    "            })\n",
    "            result_df = pd.concat([result_df, temp_df])\n",
    "        return result_df.reset_index(drop=True)\n",
    "    \n",
    "class FeaturesToTsfresh():\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    def get_time_domain_features(self):\n",
    "        sett_answ = {\n",
    "            \"signal_raw\" : \n",
    "                {\n",
    "                    \"skewness\":None,\n",
    "                    \"standard_deviation\":None,\n",
    "                    \"length\": None,\n",
    "                    \"fft_aggregated\": [{\"aggtype\": \"centroid\"},\n",
    "                                    {\"aggtype\": \"variance\"},\n",
    "                                    {\"aggtype\": \"skew\"},\n",
    "                                    {\"aggtype\": \"kurtosis\"}]\n",
    "                },\n",
    "                \n",
    "            \"signal_std32\" :\n",
    "                {\n",
    "                \"standard_deviation\": None,\n",
    "                },\n",
    "                \n",
    "            \"signal_mean256\": \n",
    "                {\n",
    "                    \"standard_deviation\": None,\n",
    "                    \"kurtosis\": None,\n",
    "                }\n",
    "            }\n",
    "        return sett_answ\n",
    "    def get_freq_domain_features(self):\n",
    "        sett_answ = EfficientFCParameters()\n",
    "        sett_answ.pop(\"fft_coefficient\", None)\n",
    "        sett_answ.pop(\"cwt_coefficients\", None)\n",
    "        sett_answ.pop(\"fft_aggregated\", None)\n",
    "        sett_answ.pop(\"number_cwt_peaks\", None)\n",
    "        sett_answ.pop(\"spkt_welch_density\", None)\n",
    "        sett_answ.pop(\"fourier_entropy\", None)\n",
    "        sett_answ.pop(\"query_similarity_count\", None)\n",
    "        sett_answ.pop(\"symmetry_looking\", None)\n",
    "        sett_answ.pop(\"large_standard_deviation\", None)\n",
    "        \n",
    "        return sett_answ\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import FeaturesToTsfresh, Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filter_column_transformer = FilterColumnCreator()\n",
    "fourier_column_transformer = FourierColumnCreator()\n",
    "\n",
    "df_ts_time = filter_column_transformer.transform(data_hdf5)\n",
    "df_ts_freq = fourier_column_transformer.transform(data_hdf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = Preprocessor()\n",
    "long_signal, df_ts_freq, df_ts_time = pr.transform(data_hdf5, from_numpy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idxs = pd.DataFrame(index=df_ts_freq[\"id\"].unique())\n",
    "X_idxs_train, X_idxs_test, y_train, y_test = train_test_split(X_idxs, label_hdf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline():\n",
    "    fft_features_extractor = Pipeline([\n",
    "            (\n",
    "                  'augmenter', RelevantFeatureAugmenter(\n",
    "                        column_id='id',\n",
    "                        column_sort='freq_num',\n",
    "                        default_fc_parameters= FeaturesToTsfresh().get_freq_domain_features(),\n",
    "                        multiclass=True,\n",
    "                        n_significant=label_hdf5.nunique(),\n",
    "                        disable_progressbar=True\n",
    "                        )\n",
    "            ),\n",
    "      ])\n",
    "    time_features_extractor = Pipeline([\n",
    "                (\n",
    "                    'augmenter', FeatureAugmenter(\n",
    "                            column_id='id',\n",
    "                            column_sort='time',\n",
    "                            kind_to_fc_parameters= FeaturesToTsfresh().get_time_domain_features(),\n",
    "                            disable_progressbar=True\n",
    "                            )\n",
    "                ),\n",
    "        ])\n",
    "\n",
    "    feature_union = FeatureUnion([\n",
    "        (\"fft_features_extractor\", fft_features_extractor),\n",
    "        (\"time_features_extractor\", time_features_extractor)\n",
    "        ])\n",
    "\n",
    "    result_ppl = Pipeline([\n",
    "        (\"feature_extraction\", feature_union),\n",
    "        (\"model\", RandomForestClassifier(\n",
    "                class_weight = \"balanced_subsample\",\n",
    "                n_estimators = 100,\n",
    "                max_depth = 12,\n",
    "                min_samples_leaf=5))\n",
    "        ])\n",
    "    return result_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_ts_freq_train = df_ts_freq[df_ts_freq[\"id\"].isin(y_train.index)]\n",
    "df_ts_time_train = df_ts_time[df_ts_time[\"id\"].isin(y_train.index)]\n",
    "\n",
    "\n",
    "result_ppl = create_pipeline()\n",
    "\n",
    "result_ppl[\"feature_extraction\"][\"fft_features_extractor\"].set_params(\n",
    "      augmenter__timeseries_container=df_ts_freq_train\n",
    "      );\n",
    "result_ppl[\"feature_extraction\"][\"time_features_extractor\"].set_params(\n",
    "      augmenter__timeseries_container=df_ts_time_train\n",
    "      );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ppl.fit(X_idxs_train, y_train)\n",
    "train_pred = result_ppl.predict(X_idxs_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      device       1.00      1.00      1.00       407\n",
      "         hit       0.99      0.98      0.99       442\n",
      "     perelaz       0.93      0.95      0.94       135\n",
      "         saw       0.99      0.98      0.99       105\n",
      "        wind       0.98      0.99      0.99       534\n",
      "\n",
      "    accuracy                           0.99      1623\n",
      "   macro avg       0.98      0.98      0.98      1623\n",
      "weighted avg       0.99      0.99      0.99      1623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_ts_freq_test = df_ts_freq[df_ts_freq[\"id\"].isin(y_test.index)]\n",
    "df_ts_time_test = df_ts_time[df_ts_time[\"id\"].isin(y_test.index)]\n",
    "\n",
    "result_ppl[\"feature_extraction\"][\"fft_features_extractor\"].set_params(augmenter__timeseries_container=df_ts_freq_test);\n",
    "result_ppl[\"feature_extraction\"][\"time_features_extractor\"].set_params(augmenter__timeseries_container=df_ts_time_test);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_pred = result_ppl.predict(X_idxs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      device       0.99      1.00      0.99       135\n",
      "         hit       0.95      0.96      0.96       146\n",
      "     perelaz       0.86      0.86      0.86        51\n",
      "         saw       0.93      0.89      0.91        46\n",
      "        wind       0.96      0.95      0.95       164\n",
      "\n",
      "    accuracy                           0.95       542\n",
      "   macro avg       0.94      0.93      0.94       542\n",
      "weighted avg       0.95      0.95      0.95       542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучаем итоговую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_ppl = create_pipeline()\n",
    "result_ppl[\"feature_extraction\"][\"fft_features_extractor\"].set_params(\n",
    "      augmenter__timeseries_container=df_ts_freq\n",
    "      );\n",
    "result_ppl[\"feature_extraction\"][\"time_features_extractor\"].set_params(\n",
    "      augmenter__timeseries_container=df_ts_time\n",
    "      );\n",
    "\n",
    "data_idxs = pd.DataFrame(index=data_hdf5[\"id\"].unique())\n",
    "result_ppl.fit(data_idxs, label_hdf5)\n",
    "\n",
    "result_ppl[\"feature_extraction\"][\"fft_features_extractor\"].set_params(\n",
    "      augmenter__timeseries_container=pd.DataFrame(index=[0])\n",
    "      );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../RnD/classifier_v3_rf/pipeline2.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('../..')\n",
    "import joblib\n",
    "joblib.dump(result_ppl, '../../RnD/classifier_v3_rf/pipeline2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
