{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ноутбук с применением pytorch для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "libpath = \"../../scripts\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from functools import cache\n",
    "\n",
    "sys.path.append(libpath)\n",
    "from preprocessing import TsfreshDatasetTransformer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Обертка для взятия данных через pytorch из директорий в структуре tsfresh\n",
    "    \"\"\"\n",
    "    def __init__(self, paths_to_nested: list[str], old_paths: list[str] = None,\n",
    "                 labels_to_delete: list[str] = None, dict_to_rename: dict[str, str] = None,\n",
    "                 balanced_classes: bool = False, output_size: int = None):\n",
    "        self.paths_to_nested = paths_to_nested\n",
    "        self.old_paths = old_paths\n",
    "        self.data_getter = TsfreshDatasetTransformer()\n",
    "        self.list_of_datasets = []\n",
    "        self.output_size = output_size\n",
    "        self.adaptive_average_pooling1d = nn.AdaptiveAvgPool1d(self.output_size)\n",
    "        for path in paths_to_nested:\n",
    "            self.list_of_datasets.append(\n",
    "                self.data_getter.make_tsfresh_structure_from_nested_directory(path)\n",
    "            )\n",
    "        if old_paths:\n",
    "            for path in old_paths:\n",
    "                self.list_of_datasets.append(\n",
    "                    self.data_getter.make_tsfresh_structure_from_simple_directory(path)\n",
    "                )\n",
    "        self.data_hdf5, self.label_hdf5 = self.concat_datasets()\n",
    "        \n",
    "        if labels_to_delete:\n",
    "            self.drop_classes(labels_to_delete)\n",
    "        if dict_to_rename:\n",
    "            self.rename_classes(dict_to_rename)\n",
    "        \n",
    "        self.data_hdf5[\"id\"] = self.rename_idxs(self.data_hdf5[\"id\"])\n",
    "        self.data_hdf5.reset_index(drop=True, inplace=True)\n",
    "        self.label_hdf5.reset_index(drop=True, inplace=True)\n",
    "        if balanced_classes:\n",
    "            self.truncate_classes()\n",
    "        \n",
    "        self.id_to_label = dict(enumerate(self.label_hdf5.unique()))\n",
    "        self.label_to_id = {i: j for j, i in self.id_to_label.items()}\n",
    "        self.indexes = self.label_hdf5.index\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.Tensor(self.data_hdf5[self.data_hdf5[\"id\"] == self.indexes[index]][\"signal_raw\"].to_numpy())\n",
    "        if self.output_size:\n",
    "            x = self.transform(x)\n",
    "        y = self.label_to_id[self.label_hdf5[self.indexes[index]]]\n",
    "        print(y)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.label_hdf5)\n",
    "    \n",
    "    def transform(self, x):\n",
    "        if x.dim() == 1:\n",
    "            return self.adaptive_average_pooling1d(x[None, :])\n",
    "        else:\n",
    "            return self.adaptive_average_pooling1d(x)\n",
    "    \n",
    "    def concat_datasets(self):\n",
    "        # Соединяет несколько датасетов\n",
    "        concated_dataframe = pd.DataFrame([])\n",
    "        concated_labels = []\n",
    "        current_id = 0\n",
    "        for (data, label) in self.list_of_datasets:\n",
    "            data_copy = data.copy()\n",
    "            data_copy[\"id\"] += current_id\n",
    "            concated_dataframe = pd.concat([concated_dataframe, data_copy], ignore_index=True)\n",
    "            current_id = concated_dataframe[\"id\"].iloc[-1] +1\n",
    "            concated_labels.extend(label)\n",
    "        return concated_dataframe, pd.Series(concated_labels)\n",
    "\n",
    "    def rename_idxs(self, array):\n",
    "        # Переименовывает колонку id после удаления метки\n",
    "        a = array.copy()\n",
    "        a = np.array(a)\n",
    "        i = 1\n",
    "        while i < len(a):\n",
    "            if a[i] - a[i-1] > 1:\n",
    "                j = i\n",
    "                constant = a[j]\n",
    "                while j < len(a) and a[j] == constant:\n",
    "                    a[j] = a[i-1] + 1\n",
    "                    j += 1\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        if a[0] > 0:\n",
    "            a -= a[0]\n",
    "        return a\n",
    "    \n",
    "    def drop_classes(self, labels_to_delete: list[str]):\n",
    "        mask_drop = self.label_hdf5.isin(labels_to_delete)\n",
    "        drop_idxs =  self.label_hdf5[mask_drop].index # id of drop labels\n",
    "\n",
    "        self.data_hdf5.drop(\n",
    "            index = self.data_hdf5[self.data_hdf5[\"id\"].isin(drop_idxs)].index,\n",
    "            inplace=True\n",
    "        )\n",
    "        self.label_hdf5.drop(\n",
    "            index = drop_idxs,\n",
    "            inplace = True\n",
    "        )\n",
    "    \n",
    "    def rename_classes(self, dict_to_rename: dict[str, str]):\n",
    "        for key, item in dict_to_rename.items():\n",
    "            self.label_hdf5.replace(key, item, inplace=True)\n",
    "\n",
    "    def truncate_classes(self):\n",
    "        idxs_after_truncate = np.array([], dtype=np.uint16) \n",
    "        interaction_count = self.label_hdf5.value_counts().min()\n",
    "\n",
    "        for interaction_name in self.label_hdf5.unique():\n",
    "            idxs_after_truncate = np.append(idxs_after_truncate, \n",
    "                                            self.label_hdf5[self.label_hdf5==interaction_name].\n",
    "                                            keys().\n",
    "                                            to_numpy(np.uint16)[:interaction_count])\n",
    "        idxs_after_truncate.sort()\n",
    "        self.label_hdf5 = self.label_hdf5.loc[idxs_after_truncate]\n",
    "        self.data_hdf5 = self.data_hdf5[self.data_hdf5[\"id\"].isin(idxs_after_truncate)]\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found unknown label in zone 455 \n",
      "date: 23/12/2024 14:57:52\n",
      "Found unknown label in zone 737 \n",
      "date: 24/12/2024 05:47:32\n",
      "Found unknown label in zone 737 \n",
      "date: 24/12/2024 05:51:41\n",
      "Found unknown label in zone 737 \n",
      "date: 24/12/2024 14:41:29\n",
      "Found unknown label in zone 737 \n",
      "date: 24/12/2024 14:41:39\n",
      "Found unknown label in zone 737 \n",
      "date: 24/12/2024 06:08:46\n",
      "Found unknown label in zone 737 \n",
      "date: 24/12/2024 16:17:34\n",
      "Found unknown label in zone 737 \n",
      "date: 24/12/2024 16:17:49\n",
      "Found unknown label in zone 775 \n",
      "date: 13/05/2025 11:39:49\n",
      "Found unknown label in zone 775 \n",
      "date: 13/05/2025 12:40:28\n",
      "Found unknown label in zone 775 \n",
      "date: 13/05/2025 12:40:34\n",
      "Found unknown label in zone 775 \n",
      "date: 13/05/2025 12:40:40\n",
      "Found unknown label in zone 775 \n",
      "date: 13/05/2025 12:40:52\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 05:57:58\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 14:04:13\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 14:23:49\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 14:25:36\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 14:28:46\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 14:29:57\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 14:33:57\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 14:37:15\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 14:45:28\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 14:47:01\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 06:05:12\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 06:06:49\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 06:29:14\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 11:52:14\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 13:03:24\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 13:59:53\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 12:37:07\n",
      "Found unknown label in zone 610 \n",
      "date: 13/05/2025 12:37:11\n"
     ]
    }
   ],
   "source": [
    "torchDataset = TorchDataset(\n",
    "    paths_to_nested=[\"/home/drozdovmk/Projects/ZB/data/data_markup/cesis_nabor_0\",\n",
    "                     \"/home/drozdovmk/Projects/ZB/data/data_markup/demostend_0\",\n",
    "                     \"/home/drozdovmk/Projects/ZB/data/data_markup/samara_wind\",\n",
    "                     \"/home/drozdovmk/Projects/ZB/data/data_markup/Kashira_13may\"],\n",
    "    old_paths=[\"/home/drozdovmk/Projects/ZB/data/data_markup/hdf5_adaptive\"],\n",
    "    labels_to_delete=[\"unknown\", \"hit_series\"],\n",
    "    dict_to_rename={\"hit_g\" : \"hit\", \"hit_z\": \"hit\"},\n",
    "    balanced_classes = False,\n",
    "    output_size=1000\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(torchDataset))  # 70% под обучение\n",
    "val_size = int(0.15 * len(torchDataset))   # 15% под валидацию\n",
    "test_size = len(torchDataset) - train_size - val_size  # остальное под тест\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    torchDataset, \n",
    "    [train_size, val_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Предположим, у нас есть модель, даталоадеры и функция потерь\n",
    "model = ...  # Ваша модель (nn.Module)\n",
    "train_loader = ...  # DataLoader для обучающих данных\n",
    "val_loader = ...  # DataLoader для валидационных данных (опционально)\n",
    "criterion = ...  # Функция потерь (например, nn.CrossEntropyLoss())\n",
    "optimizer = ...  # Оптимизатор (например, optim.Adam(model.parameters()))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Режим обучения\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        # Перенос данных на устройство (GPU/CPU)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Статистика\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Можно выводить прогресс\n",
    "        if batch_idx % 100 == 99:  # Печатаем каждые 100 батчей\n",
    "            print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Валидация после эпохи (опционально)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, targets).item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1} completed. Train Loss: {running_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "print('Training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5., 5.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.Tensor([[1,2,3,4,5]])\n",
    "torch.nn.functional.pad(x, pad=(0,2-1), value=x[:,-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ф"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
